{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado del dataset de TW de las noticias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a tener los siguientes conjuntos de datos\n",
    "\n",
    "ds_completo: noticias + tw completos con el clasificador JM aplicado  \n",
    "\n",
    "ds_tw: tw del dataset completo con el clasificador JM aplicado (sin el contexto)  \n",
    "ds_train: dataset de entrenamiento de noticias + tw con el etiquetado  \n",
    "ds_test: dataset de test con el etiquetado    \n",
    "ds_dev: dataset de validacion con el etiquetado  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"piubamas/articles_and_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_completo = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ds_completo tiene 537.201 noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_completo[200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 separo y preparo el datasets solo con comentarios\n",
    "\n",
    "No guardo la informaci√≥n de la noticia, solo el id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_arg = []\n",
    "\n",
    "for noticia in tqdm(dataset['train']):\n",
    "\n",
    "    date_noticia = noticia[\"created_at\"]\n",
    "    # Convertimos la fecha de la noticia a un objeto de python\n",
    "    datenoticia = datetime.strptime(date_noticia, \"%Y-%m-%dT%H:%M:%S%fZ\")\n",
    "\n",
    "    for comment in noticia[\"comments\"]: \n",
    "        date_comment = comment[\"created_at\"]\n",
    "        # Convertimos la fecha del comentario a un objeto de python\n",
    "        datecomment = datetime.strptime(date_comment, \"%Y-%m-%dT%H:%M:%S%fZ\")\n",
    "        # anexa comentarios de diarios argentinos\n",
    "        tweets_arg.append({\n",
    "            \"tweet_id_noticia\": noticia[\"tweet_id\"],\n",
    "            #\"user_noticia\": noticia[\"user\"], \n",
    "           # \"date_noticia\": datenoticia, \n",
    "           # \"title_noticia\": noticia[\"title\"], \n",
    "            #\"url_noticia\": noticia[\"url\"], \n",
    "            \"tweet\": comment[\"text\"],\n",
    "            \"date_tweet\": datecomment,\n",
    "            \"APPEARANCE\":comment[\"prediction\"][\"APPEARANCE\"], \n",
    "            \"CLASS\":comment[\"prediction\"][\"CLASS\"], \n",
    "            \"CRIMINAL\":comment[\"prediction\"][\"CRIMINAL\"], \n",
    "            \"DISABLED\":comment[\"prediction\"][\"DISABLED\"], \n",
    "            \"LGBTI\":comment[\"prediction\"][\"LGBTI\"], \n",
    "            \"POLITICS\":comment[\"prediction\"][\"POLITICS\"], \n",
    "            \"RACISM\":comment[\"prediction\"][\"RACISM\"], \n",
    "            \"WOMEN\":comment[\"prediction\"][\"WOMEN\"], \n",
    "            \"CALLS\":comment[\"prediction\"][\"CALLS\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Levanto el dataset de train, test y validacion en parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train=pd.read_parquet('train/train-00000-of-00001.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test=pd.read_parquet('train/test-00000-of-00001.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dev=pd.read_parquet('train/dev-00000-of-00001.parquet', engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
